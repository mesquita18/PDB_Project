{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWtZmjxKf9tT701rVxLWE8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mesquita18/PDB_Project/blob/main/Atividade_RP_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nessa primeira solução, vamos levar em consideração o algoritmo sequencial Batch GD. Nela, tentaremos minimizar o erro, usando a função MSE. Para isso, atualizaremos os valores dos coeficientes w's a cada iteração(epoch)."
      ],
      "metadata": {
        "id": "uMP28VMFRPBr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWaYBp9kO7XL",
        "outputId": "f1ffbd9a-59d8-4330-844d-daf770d976a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w1 = 0.9999999999989218, w2 = -2.000000000000353 e b = 5.658592369121521e-12\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dados do problema\n",
        "X = np.array([[4, 1],[2, 8],[1, 0],[3, 2],[1, 4],[6, 7]])\n",
        "\n",
        "# Valores desejados para cada dado de treinamento\n",
        "y = np.array([2, -14, 1, -1, -7, -8])\n",
        "n = len(y)\n",
        "\n",
        "w1, w2, b = 0.0, 0.0, 0.0 # Inicialização dos parâmetros\n",
        "alpha = 0.01  # taxa de aprendizado\n",
        "epochs = 10000  # número de iterações\n",
        "history = []  # Guarda o erro ao longo das iterações(epochs)\n",
        "\n",
        "# Gradiente descendente\n",
        "for _ in range(epochs):\n",
        "    y_pred = w1 * X[:, 0] + w2 * X[:, 1] + b\n",
        "    error = y_pred - y\n",
        "\n",
        "    # Gradientes\n",
        "    dw1 = (1/n) * np.sum(error * X[:, 0])\n",
        "    dw2 = (1/n) * np.sum(error * X[:, 1])\n",
        "    db  = (1/n) * np.sum(error)\n",
        "\n",
        "    # Atualização dos parâmetros\n",
        "    w1 -= alpha * dw1\n",
        "    w2 -= alpha * dw2\n",
        "    b  -= alpha * db\n",
        "\n",
        "    # Armazenar o custo atual (MSE)\n",
        "    cost = (1/(2*n)) * np.sum(error ** 2)\n",
        "    history.append(cost)\n",
        "\n",
        "# Resultados finais dos coeficientes w1, w2, b\n",
        "print(f\"w1 = {w1}, w2 = {w2} e b = {b}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nessa segunda solução, vamos levar em consideração o algoritmo sequencial Stochastic GD. Nela, tentaremos minimizar o erro, usando a função MSE. Porém, diferentemente do Batch, que usa todo o conjunto de dados para calcular os gradientes em cada epoch, a atualização dos pesos é feita um exemplo por vez."
      ],
      "metadata": {
        "id": "VGJEqNt3U-29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dados do problema\n",
        "X = np.array([[4, 1],[2, 8],[1, 0],[3, 2],[1, 4],[6, 7]])\n",
        "\n",
        "# Valores desejados para cada dado de treinamento\n",
        "y = np.array([2, -14, 1, -1, -7, -8])\n",
        "n = len(y)\n",
        "\n",
        "w1, w2, b = 0.0, 0.0, 0.0 # Inicialização dos parâmetros\n",
        "alpha = 0.01  # taxa de aprendizado\n",
        "epochs = 10000  # número de iterações\n",
        "history = []  # Guarda o erro ao longo das iterações(epochs)\n",
        "\n",
        "# Gradiente descentende\n",
        "for epoch in range(epochs):\n",
        "    for i in range(n):  # n = número de amostras\n",
        "        # calcular erro para apenas um exemplo\n",
        "        y_pred = w1 * X[i, 0] + w2 * X[i, 1] + b\n",
        "        error = y_pred - y[i]\n",
        "\n",
        "        # atualizar os parâmetros com base nesse único erro\n",
        "        w1 -= alpha * error * X[i, 0]\n",
        "        w2 -= alpha * error * X[i, 1]\n",
        "        b  -= alpha * error\n",
        "\n",
        "    # Armazenar o custo atual (MSE)\n",
        "    y_preds = X[:, 0] * w1 + X[:, 1] * w2 + b\n",
        "    epoch_error = y_preds - y\n",
        "    cost = (1/(2*n)) * np.sum(epoch_error ** 2)\n",
        "    history.append(cost)\n",
        "\n",
        "# Resultados finais dos coeficientes w1, w2, b\n",
        "print(f\"w1 = {w1:.3f}, w2 = {w2:.3f}, b = {b:.3f}, erro = {history[-1]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA9FAcptWDQk",
        "outputId": "e758c847-8204-4c0b-e964-9db03da4a23c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w1 = 1.000, w2 = -2.000, b = 0.000, erro = 0.0000\n"
          ]
        }
      ]
    }
  ]
}